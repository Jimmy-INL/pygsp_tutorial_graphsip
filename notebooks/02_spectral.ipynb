{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from pygsp import graphs\n",
    "from additional_utils import create_SBM, AR_index, generate_concentric_circles\n",
    "import networkx as nx\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Spectral Graph Theory\n",
    "\n",
    "We'll see here two of the most celebrated algorithms stemming from Spectral Graph Theory:\n",
    "1. [Spectral clustering](https://en.wikipedia.org/wiki/Spectral_clustering): clustering data using a similarity graphs\n",
    "1. [Laplacian eigenmaps](https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction#Laplacian_eigenmaps): dimensionality reduction which preserves geodesic distances\n",
    "\n",
    "**TODO**: look at <https://nbviewer.jupyter.org/github/mdeff/ntds_2017/blob/outputs/assignments/03_solution.ipynb>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 The spectrum of a graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider here only undirected graphs, such that the Laplacian matrix is real symmetric, thus diagonalizable in an orthonormal eigenbasis\n",
    "$$\\mathbf{L}=\\mathbf{U}\\mathbf{\\Lambda U}^\\top,$$\n",
    "where $\\mathbf{U}=(\\mathbf{u}_1|\\ldots|\\mathbf{u}_N)\\in\\mathbb{R}^{N\\times N}$ is the matrix of orthonormal eigenvectors and $\\mathbf{\\Lambda} = \\text{diag}(\\lambda_1,\\ldots,\\lambda_N)$ is the diagonal matrix of associated sorted eigenvalues:\n",
    "$$\\lambda_1\\leq\\lambda_2\\leq\\ldots\\leq\\lambda_N.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that $\\lambda_1$ is necessarily $0$ and that $\\lambda_2>0$ iff the graph is connected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = graphs.Community(N=100, Nc=5)\n",
    "G.plot()\n",
    "eig_val, U = sp.linalg.eigh(G.L.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at ``` eig_val```. Look at ```np.finfo(float).resolution```. How many connected components are in the graph? Check with the networkx function nx.connected_components ```G_nx = nx.from_numpy_matrix(G.A.toarray()); sorted(nx.connected_components(G_nx), key = len, reverse=True)```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we used ```sp.linalg.eigh``` to diagonalize $\\mathbf{L}$ because it is the eigensolver optimized for symmetric matrices. Also, ```sp.linalg.eigh``` cannot take sparse matrices as entries. For sparse matrices, optimized eigensolvers exist to obtain the first few eigenvalues and eigenvectors, such as "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_val, U = sp.sparse.linalg.eigsh(G.L, k=10, which='SM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use option ```which='SM'``` for smallest magnitude search, and ```which='LM'``` for largest magnitude search. ```eigsh``` is nevertheless only made to compute only a few eigenvectors. You will quickly see the utility of ```sp.sparse.linalg.eigsh(G.L, k=10, which='SM')``` versus ```sp.linalg.eigh(G.L.toarray())``` if you increase the size of the network to $10^4$ nodes or larger for instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. The Stochastic Block Model (SBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Stochastic Block Model (SBM) is a latent variable model of structured random graphs. \n",
    "* The set of $N$ nodes is partitioned in $k$ blocks $B_1, \\ldots, B_k$ : $\\cup_{i} B_i = \\{1,\\ldots,N\\}$ and $\\forall i\\neq j, B_i\\cap B_j = \\emptyset$.\n",
    "* We define a probability matrix $\\mathbf{P}\\in\\mathbb{R}^{k\\times k}$ such that $P_{ij}$ is the probability of connection between a node in $B_i$ and a node in $B_j$. \n",
    "* To generate an instance of SBM: for each pair of nodes in the graph, draw a Bernoulli random variable of parameter corresponding to their respective blocks.\n",
    "\n",
    "To simplify, we will suppose that all intra-block probabilities are set to $p_{\\text{in}}$ and all inter-block probabilities are set to $p_{\\text{out}}$. Also, we will consider only balanced SBMs (ie SBMs whose blocks have same size).\n",
    "\n",
    "In the case of balanced SBMs, instead of the pair of parameters $(p_{\\text{in}}, p_{\\text{out}})$, we can consider the pair $(c,\\epsilon)$ where $c$ is the average degree of the graph and $\\epsilon = \\frac{p_{\\text{out}}}{p_{\\text{in}}}$ is a measure of fuzziness: the larger $\\epsilon$, the fuzzier the community structure. The relations between both pairs of parameters are:\n",
    " $$p_{\\text{in}} = \\frac{kc}{N-k+(k-1)\\epsilon N}~~~\\text{ and }~~~p_{\\text{out}} = \\frac{kc\\epsilon}{N-k+(k-1)\\epsilon N}$$\n",
    "Note that, for a fixed triplet $(\\epsilon, N, k)$, $c$ is not a free parameter. \n",
    "\n",
    "**The classical inference SBM problem** is: given an instance of an SBM $G$, infer the blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Impossibility threshold\n",
    "In the balanced (ie: all blocks have the same size) and sparse (ie average degree $c=\\mathcal{O}(1)$) case, the following phase transition has been shown ([Decelle et al.](https://arxiv.org/pdf/1109.3041.pdf), [MassouliÃ©](https://arxiv.org/pdf/1311.3085.pdf), [Abbe et al.](https://arxiv.org/pdf/1503.00609.pdf)). For a given pair $(k, c)$, there exists a critical fuzziness $\\epsilon_c$ such that as $N\\rightarrow \\infty$:\n",
    "* if $\\epsilon>\\epsilon_c$, there exists no algorithm capable of extracting any structural information.\n",
    "* if $\\epsilon<\\epsilon_c$, it is *theoretically* possible to retrieve some kind of structural information\n",
    "\n",
    "with $\\epsilon_c = \\frac{c - \\sqrt{c}}{c + \\sqrt{c} (k-1)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 500; # number of nodes\n",
    "k = 4; # number of blocks\n",
    "c = 16; # average degree\n",
    "\n",
    "com_size = np.ones(k) * (N/k)\n",
    "com_size = com_size.astype(int)\n",
    "\n",
    "epsi_c = (c - np.sqrt(c)) / (c + np.sqrt(c) * (k-1)); # critical fuzziness\n",
    "epsi = epsi_c / 40; # this is a very strong block structure\n",
    "\n",
    "A, truth = create_SBM(N, k, c, epsi, com_size)\n",
    "G = graphs.Graph(A)\n",
    "G.truth = truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With ```epsi = epsi_c / 40```, ie, a very strong block structure, blocks appear even 'visually' with spring-based plotting algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.set_coordinates(kind='spring') #here with a spring based algorithm\n",
    "G.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. The Eigengap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first few eigenvalues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_val, U = sp.sparse.linalg.eigsh(G.L, k=50, which='SM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(eig_val, '+-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The large gap between $\\lambda_k$ and $\\lambda_{k+1}$ is called the \"spectral gap\": it heuristically appears when there is a strong structure in $k$ communities (even though there exist some theoretical justifications)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: eigengap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plot a SBM graph with the spring-based plotting algorithm with $\\epsilon=\\epsilon_c/3$ for instance: one cannot extract communities 'visually' !\n",
    "* Let's formally define the normalized spectral gap as $\\Gamma = \\frac{\\lambda_{k+1}-\\lambda_k}{\\lambda_k}$. Plot the average of $\\Gamma$ as a function of the fuziness $\\epsilon$ in the case of balanced SBM graphs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 The first few eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen that an eigengap usually exists in community-structured graphs. What can we say about the first few eigenvectors? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot them in a balanced SBM graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 500; # number of nodes\n",
    "k = 4; # number of blocks\n",
    "c = 16; # average degree\n",
    "\n",
    "com_size = np.ones(k) * (N/k)\n",
    "com_size = com_size.astype(int)\n",
    "\n",
    "epsi_c = (c - np.sqrt(c)) / (c + np.sqrt(c) * (k-1)); # critical fuzziness\n",
    "epsi = epsi_c / 40; # this is a very strong block structure\n",
    "\n",
    "A, truth = create_SBM(N, k, c, epsi, com_size)\n",
    "G = graphs.Graph(A)\n",
    "G.truth = truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_val, U = sp.sparse.linalg.eigsh(G.L, k=10, which='SM')\n",
    "G.set_coordinates(kind='spring')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.plot_signal(U[:,0])\n",
    "G.plot_signal(U[:,1])\n",
    "G.plot_signal(U[:,2])\n",
    "G.plot_signal(U[:,3])\n",
    "G.plot_signal(U[:,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that:\n",
    "$$\\lambda_k=\\mathbf{u}_k^\\top\\mathbf{L}\\mathbf{u}_k=\\sum_{i\\sim j}\\mathbf{W}_{ij} (u_k(i)-u_k(j))^2,$$\n",
    "such that eigenvectors associated to low eigenvalues tend to be smooth with respect to any path in the network. In block-structured graphs, this usually means quasi-constant within each block. Spectral clustering takes advantage of this property. Let us look at a 3-block structure, for better illustration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 300; # number of nodes\n",
    "k = 3; # number of blocks\n",
    "c = 16; # average degree\n",
    "\n",
    "com_size = np.ones(k) * (N/k)\n",
    "com_size = com_size.astype(int)\n",
    "\n",
    "epsi_c = (c - np.sqrt(c)) / (c + np.sqrt(c) * (k-1)); # critical fuzziness\n",
    "epsi = epsi_c / 40; # this is a very strong block structure\n",
    "\n",
    "A, truth = create_SBM(N, k, c, epsi, com_size)\n",
    "G = graphs.Graph(A)\n",
    "G.truth = truth\n",
    "\n",
    "eig_val, U = sp.sparse.linalg.eigsh(G.L, k=10, which='SM')\n",
    "G.set_coordinates(kind='spring')\n",
    "\n",
    "G.plot_signal(U[:,0])\n",
    "G.plot_signal(U[:,1])\n",
    "G.plot_signal(U[:,2])\n",
    "G.plot_signal(U[:,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we want to recover in which block belongs each node. The first eigenvector's information is useless (as long as we use the combinatorial Laplacian anyways) as it is constant. A solution is to plot each node $i$ in 2D with coordinates $(u_2(i), u_3(i))$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(U[:,1], U[:,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: spectral clustering\n",
    "\n",
    "* Show that, in the last example, performing $k$-means in the $2D$ plane defined by $\\mathbf{u}_2$ and $\\mathbf{u}_3$ recovers the 3 block structures (the ground truth is in ```G.truth```). For $k$-means, use the function ```KMeans``` in the toolbox ```sklearn.cluster```. To measure performance, you can use the function ```AR_index``` which computes the Adjusted Rand index (both functions were already imported in the preamble)\n",
    "* Given the intuition gained, write a general spectral clustering algorithm that takes as entry a graph $G$, a number of communities $k$, and outputs its community structure in $k$ blocks. \n",
    "* Plot the average performance of community mining with this spectral clustering algorithm on balanced SBMs versus the fuziness $\\epsilon$. \n",
    "* To go further, you can read [\"A tutorial on spectral clustering\" by U. von Luxburg](https://arxiv.org/pdf/0711.0189.pdf). You will learn among other things that a degree-corrected version is in general preferable (using the normalized Laplacian instead of the combinatorial Lap for instance). For arguments to choose the \"best\" degree-correction, see ['Improved spectral community detection in large heterogeneous networks' by Tiomoko Ali and Couillet](http://www.jmlr.org/papers/volume18/17-247/17-247.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Creating the graph\n",
    "We have seen the spectral clustering algorithm with input a graph $G$. In many cases, however, the input data is simply $N$ vectors in dimension $d$. The construction of the graph is a problem in itself! Let us look at a toy point cloud example where spectral clustering is useful: two concentric circles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_in=300 # number of nodes inside\n",
    "N_out=500 # number of nodes outside\n",
    "sigma_in= 0.15\n",
    "sigma_out= 0.15\n",
    "data, truth = generate_concentric_circles(N_in, N_out, sigma_in, sigma_out)\n",
    "plt.figure()\n",
    "plt.scatter(data[:,0], data[:,1], c=truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$k$-means will obviously not work on this example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: which similarity graph should one create from the data \n",
    "Look at Notebook 1 for the two main ways of creating a graph: $k$-nearest neighbours or $\\epsilon$-proximity. \n",
    "* What is one of the main differences between these two techniques? (regarding the degree distribution for instance) \n",
    "* How does the performance change vs the choice of $k$ (the number of neighbours in the kNN graph)? \n",
    "* How does the perf change vs the choice of $\\epsilon$ (in the $\\epsilon$-graph)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Laplacian eigenmaps\n",
    "\n",
    "**TODO**:\n",
    "* mathematical formulation of the problem\n",
    "* solution is the first eigenvectors of the Laplacian\n",
    "* implement with pygsp\n",
    "* need data with labels: FMA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
